{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassPipeline.ipynb",
      "provenance": [],
      "mount_file_id": "1SAQa4S1n3rnyaDigEYWkt71yLCLqXtPw",
      "authorship_tag": "ABX9TyPkp9UeJfTvVQqRSyK0M6cF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CherylA24/CreditScoringClassification/blob/main/ClassPipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_IoYmAVBCRb",
        "outputId": "af98287c-9cf6-4f71-ef6d-3b298cd405bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.4.1-py2.py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 20 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G63ALuWAgf69",
        "outputId": "7c35eeed-0bdd-4f3f-d640-53f22b35f506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dython in /usr/local/lib/python3.7/dist-packages (0.7.1.post3)\n",
            "Requirement already satisfied: pandas>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from dython) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from dython) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from dython) (1.21.6)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from dython) (0.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from dython) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.4.3 in /usr/local/lib/python3.7/dist-packages (from dython) (3.5.2)\n",
            "Requirement already satisfied: scikit-plot>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from dython) (0.3.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (7.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (4.33.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (21.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4.3->dython) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.4.3->dython) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.2->dython) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.4.3->dython) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->dython) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->dython) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "K-UomOKewLu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_score, f1_score, recall_score\n",
        "from sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectKBest\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler, QuantileTransformer, LabelEncoder, OneHotEncoder\n",
        "import category_encoders as ce\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, IterativeImputer\n",
        "from sklearn.feature_selection._base import SelectorMixin\n",
        "from sklearn.utils.sparsefuncs import mean_variance_axis, min_max_axis\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "from sklearn.utils._tags import _safe_tags\n",
        "from sklearn.utils import (\n",
        "    check_array,\n",
        "    safe_mask,\n",
        "    safe_sqr,\n",
        ")\n",
        "# from dython.nominal import associations\n",
        "\n",
        "import joblib\n",
        "from os.path import exists as file_exists\n",
        "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
        "import itertools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOGuNgxlwcrp",
        "outputId": "3b397337-ff15-4fa0-e9cc-5917716f9f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FillCatsNA(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X, y = None):\n",
        "    X_data = X.copy()\n",
        "\n",
        "    X_data['jenispinjaman'] = X_data['jenispinjaman'].fillna(value='-')\n",
        "    X_data['JarakDomisili'] = X_data['JarakDomisili'].fillna(value = 'Kurang atau Sama Dengan 60 KM')\n",
        "\n",
        "    X_data = X_data.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    # X_data = X_data.fillna(value='-')\n",
        "\n",
        "    return X_data "
      ],
      "metadata": {
        "id": "EWsGdkEj9Q6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvertData(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X):\n",
        "    X_data = X.copy()\n",
        "\n",
        "    X_data['TotalLamaPernahBekerja'] = X_data['TotalLamaPernahBekerja'].fillna(value = 0)\n",
        "\n",
        "    def repeated(string):\n",
        "      for x in range(1, len(string)):\n",
        "        substring = string[:x]\n",
        "        if substring * (len(string)//len(substring))+(substring[:len(string)%len(substring)]) == string:\n",
        "          return substring\n",
        "        elif (string == '2015'):\n",
        "          return '215'\n",
        "        elif (string == '2016'):\n",
        "          return '216'\n",
        "        elif (string == '1994'):\n",
        "          return '228'\n",
        "        elif (string =='21552263233'):\n",
        "          return '1'\n",
        "        else :\n",
        "          return string[0:3]\n",
        "\n",
        "      return string\n",
        "\n",
        "    X_data['TotalLamaPernahBekerja'] = X_data['TotalLamaPernahBekerja'].astype(str)\n",
        "    X_data['TotalLamaPernahBekerja'] = X_data['TotalLamaPernahBekerja'].str.extract('(\\d+)')\n",
        "    X_data['TotalLamaPernahBekerja'] = X_data['TotalLamaPernahBekerja'].astype(str)\n",
        "    X_data['TotalLamaPernahBekerja'] = X_data['TotalLamaPernahBekerja'].apply(repeated)\n",
        "    X_data['TotalLamaPernahBekerja'] = X_data['TotalLamaPernahBekerja'].astype(float)\n",
        "\n",
        "    return X_data"
      ],
      "metadata": {
        "id": "Gy4mc9uKEQl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseVarianceThreshold(SelectorMixin, BaseEstimator):\n",
        "\n",
        "  def __init__(self, threshold=0.0):\n",
        "    self.threshold = threshold\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "\n",
        "    X = self._validate_data(\n",
        "        X,\n",
        "        accept_sparse=(\"csr\", \"csc\"),\n",
        "        dtype=np.float64,\n",
        "        force_all_finite=\"allow-nan\",\n",
        "    )\n",
        "\n",
        "    if hasattr(X, \"toarray\"):  # sparse matrix\n",
        "      _, self.variances_ = mean_variance_axis(X, axis=0)\n",
        "      if self.threshold == 0:\n",
        "        mins, maxes = min_max_axis(X, axis=0)\n",
        "        peak_to_peaks = maxes - mins\n",
        "    else:\n",
        "      self.variances_ = np.nanvar(X, axis=0)\n",
        "      if self.threshold == 0:\n",
        "        peak_to_peaks = np.ptp(X, axis=0)\n",
        "\n",
        "    if self.threshold == 0:\n",
        "      compare_arr = np.array([self.variances_, peak_to_peaks])\n",
        "      self.variances_ = np.nanmin(compare_arr, axis=0)\n",
        "    elif self.threshold < 0.0:\n",
        "      raise ValueError(f\"Threshold must be non-negative. Got: {self.threshold}\")\n",
        "\n",
        "    if np.all(~np.isfinite(self.variances_) | (self.variances_ <= self.threshold)):\n",
        "      msg = \"No feature in X meets the variance threshold {0:.5f}\"\n",
        "      if X.shape[0] == 1:\n",
        "        msg += \" (X contains only one sample)\"\n",
        "      raise ValueError(msg.format(self.threshold))\n",
        "\n",
        "    return self\n",
        "\n",
        "  def _get_support_mask(self):\n",
        "    check_is_fitted(self)\n",
        "\n",
        "    return self.variances_ < self.threshold\n",
        "\n",
        "  def transform(self, X):\n",
        "    X = self._validate_data(\n",
        "        X,\n",
        "        dtype=None,\n",
        "        accept_sparse=\"csr\",\n",
        "        force_all_finite=not _safe_tags(self, key=\"allow_nan\"),\n",
        "        reset=False,\n",
        "    )\n",
        "    return self._transform(X)\n",
        "\n",
        "  def _transform(self, X):\n",
        "    mask = self.get_support()\n",
        "    if not mask.any():\n",
        "      warn(\n",
        "          \"No features were selected: either the data is\"\n",
        "          \" too noisy or the selection test too strict.\",\n",
        "          UserWarning,\n",
        "      )\n",
        "      return np.empty(0).reshape((X.shape[0], 0))\n",
        "    if len(mask) != X.shape[1]:\n",
        "      raise ValueError(\"X has a different shape than during fitting.\")\n",
        "    return X[:, safe_mask(X, mask)]"
      ],
      "metadata": {
        "id": "8GA6BRM4Q8va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropCorrelation(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, threshold):\n",
        "    self.threshold = threshold\n",
        "    self.col_corr = None\n",
        "  \n",
        "  def fit(self, X, y = None):\n",
        "    col_corr = set()\n",
        "    X = pd.DataFrame(X)\n",
        "    X['FLAG_BAD'] = y\n",
        "    corr_matrix = X.corr()\n",
        "    target_loc = corr_matrix.columns.get_loc(\"FLAG_BAD\")\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "      for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j])> self.threshold:\n",
        "          if (abs(corr_matrix.iloc[i, target_loc]) < abs(corr_matrix.iloc[j, target_loc])):\n",
        "            colname = corr_matrix.columns[j]\n",
        "            col_corr.add(colname)\n",
        "          else:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            col_corr.add(colname)\n",
        "    \n",
        "    # col_corr.add('FLAG_BAD')\n",
        "    self.col_corr = col_corr\n",
        "\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X, y=None, **kwargs):\n",
        "    return (pd.DataFrame(X)).drop(labels = self.col_corr, axis=1)"
      ],
      "metadata": {
        "id": "dkiImV74Eo7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "  c = itertools.count()\n",
        "\n",
        "  df = pd.read_csv('/content/drive/MyDrive/Data Marginal/MarginalUpd(1).csv')\n",
        "  df[\"FLAG_BAD\"] = np.where((df[\"ever30plus\"] == 'a. Ever 30Plus') | (df[\"FlagNegCust\"] == '1. NegativeCustomer') , 1, 0)\n",
        "\n",
        "  cats_cols = ['PersonalType','Gender','MaritalStatus', 'ResidenceCompanyProvince', 'HomeCompanyStatus',\n",
        "              'Pekerjaan','jabatan','Status_pekerjaan','Jenis_pekerjaan2', 'Pendidikan','HomeFixedLine',\n",
        "              'KantorFixedLine','Wayofpayment','IsSyariah','ProductType','UsedNew', 'GroupProductId',\n",
        "              'applicationsource','FirstInstallment','AssetCategory','AssetUsage','MainCoverage', 'GRPNTF',\n",
        "              'GRPOTR','Bidang_usaha','flagImpact','isoveride', 'ProductOffering', 'GRPDP', 'SektorEkonomi',\n",
        "              'SupplierType', 'SupplierCategory','InstallmentScheme','AssetType','AssetCategoryId','FlagMerkPopular',\n",
        "              'MadeIn','isKPM','Jenis_KPM', 'jenispinjaman', 'PaymentMethod','FlagRO','JarakDomisili','FlagNegCust',\n",
        "              'CompanyCity','ResidenceCompanyCity', 'Nama_Perusahaan','Supplier']\n",
        "  num_cols = ['ResidenceCompanyZipCode', 'Lama_Menempati_Tahun','Lama_Menempati_Bulan', 'employmentsinceyear',\n",
        "              'CompanyZipCode', 'NumOfDependence', 'NTFIDR', 'TotalOTR', 'Pendapatan', \n",
        "              'DPPercentage', 'CAMOSObligorExposure', 'StartingBalance', 'AvgDebit', 'TotalLamaPernahBekerja',\n",
        "              'AvgCredit','AvgBalance', 'LastBalanceAmt','GrowthBalance', 'jmltunggakanpokok','jmltunggakanhari']\n",
        "\n",
        "  target_var=['FLAG_BAD']\n",
        "  all_var = cats_cols + num_cols + target_var\n",
        "\n",
        "  df = df[df.columns[df.columns.isin(all_var)]]\n",
        "\n",
        "  X = df.drop(\"FLAG_BAD\",axis=1)\n",
        "  y = df[\"FLAG_BAD\"]\n",
        "    \n",
        "  X_train, X_test, y_train, y_test= train_test_split(X, y, test_size = 0.3, random_state=0, stratify = y)\n",
        "  \n",
        "  def unique_file(basename, ext):\n",
        "    actualname = \"%s.%s\" % (basename, ext)\n",
        "    while file_exists(actualname):\n",
        "      actualname = \"%s (%d).%s\" % (basename, next(c), ext)\n",
        "    return actualname\n",
        "\n",
        "  def oldFile(basename, ext):\n",
        "    old_file = \"%s (%d).%s\" % (basename, next(c)-2 , ext)\n",
        "    return old_file\n",
        "\n",
        "  models = {\n",
        "      'GaussianNB' : GaussianNB(),\n",
        "      'LogisticRegression' : LogisticRegression(fit_intercept = True, solver = 'saga', max_iter = 5000),\n",
        "      'RandomForestClassifier' : RandomForestClassifier(n_estimators=100, bootstrap = True, oob_score = True, random_state = 42),\n",
        "      'XGBClassifier' : xgb.XGBClassifier(objective=\"binary:logistic\", random_state = 42),\n",
        "      'AdaBoostClassifier' : AdaBoostClassifier(random_state=42),\n",
        "      'LGBMClassifier' : lgb.LGBMClassifier(random_state = 42)\n",
        "  }\n",
        "\n",
        "  model_score = pd.DataFrame(\n",
        "    columns = ['Model Name','Train Accuracy','Test Accuracy','ROC Value','Precision','Recall','F1 Score'])\n",
        "\n",
        "  cat_transformer = Pipeline(\n",
        "    steps=[('imputer', fcna),\n",
        "          ('encoder2', OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "          ('vt1', rvt),\n",
        "          ('vt2', VarianceThreshold(threshold = 0.01)),\n",
        "          ('encoder', ce.WOEEncoder())]\n",
        "  )\n",
        "\n",
        "  num_transformer = Pipeline(\n",
        "    steps = [('convert',cd),\n",
        "            ('imputer', SimpleImputer(missing_values = np.nan, strategy='constant', fill_value=0.0)),\n",
        "            ('scaler', QuantileTransformer(output_distribution='normal', n_quantiles = 100000))]\n",
        "  )\n",
        "\n",
        "  preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', num_transformer, num_cols),\n",
        "                  ('cats', cat_transformer, cats_cols)]\n",
        "  )\n",
        "\n",
        "  fitted_models = []\n",
        "  predictions = []\n",
        "\n",
        "  file_model_metrics = unique_file(\"ModelMetrics\",\"txt\")\n",
        "\n",
        "  for key in models:\n",
        "    pipe = Pipeline(\n",
        "        steps=[('preprocessor',preprocessor),\n",
        "              ('dropcorr',dc),\n",
        "              ('features',SelectKBest(mutual_info_classif, k=20)),\n",
        "              ('model', models[key])]\n",
        "    )\n",
        "\n",
        "    model = pipe.fit(X_train, y_train)\n",
        "    fitted_models.append(model)\n",
        "\n",
        "    y_preds = model.predict(X_test)\n",
        "    predictions.append(y_preds)\n",
        "\n",
        "    y_probs = model.predict_proba(X_test)[:, 1] \n",
        "\n",
        "    train_acc = (1 - (1 - model.score(X_train, y_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))\n",
        "    test_acc = (1 - (1 - model.score(X_test, y_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "\n",
        "    with open(file_model_metrics, 'a') as outfile:\n",
        "      outfile.write(\"\\n\")\n",
        "      outfile.write(key)\n",
        "      outfile.write(\"\\n====================\\n\")\n",
        "      outfile.write(\"train accuracy: {:.2f}%\\n\".format(train_acc * 100))\n",
        "      outfile.write(\"test accuracy: {:.2f}%\\n\".format(test_acc * 100))\n",
        "      outfile.write(\"roc value : {:.2f}%\\n\".format((roc_auc_score(y_test, y_probs))*100))\n",
        "      outfile.write('\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n'.format(classification_report(y_test, y_preds),confusion_matrix(y_test, y_preds)))\n",
        "\n",
        "    row = {\n",
        "        'Model Name': key,\n",
        "        'Train Accuracy' : train_acc,\n",
        "        'Test Accuracy' : test_acc,\n",
        "        'ROC Value' : roc_auc_score(y_test,y_probs),\n",
        "        'Precision' : precision_score(y_test,y_preds),\n",
        "        'Recall' : recall_score(y_test, y_preds),\n",
        "        'F1 Score' : f1_score(y_test, y_preds)\n",
        "      }\n",
        "\n",
        "    model_score = model_score.append(row, ignore_index = True)\n",
        "\n",
        "  model_score.set_index(\"Model Name\", inplace = True)\n",
        "  model_score = model_score.T\n",
        "  model_score['Best Score'] = model_score.idxmax(axis=1)\n",
        "  model_score.to_excel(unique_file(\"ModelScore\", \"xlsx\"))\n",
        "  # model_score.to_csv(unique_file(\"ModelScore\", \"csv\"))\n",
        "  model_score\n",
        "\n",
        "  if (model_score.loc['ROC Value']['Best Score'] == 'GaussianNB'):\n",
        "    best_model = fitted_models[0]\n",
        "    best_roc_value = model_score.loc['ROC Value']['GaussianNB']\n",
        "    best_preds = predictions[0]\n",
        "\n",
        "  elif (model_score.loc['ROC Value']['Best Score'] == 'LogisticRegression'):\n",
        "    best_model = fitted_models[1]\n",
        "    best_roc_value = model_score.loc['ROC Value']['LogisticRegression']\n",
        "    best_preds = predictions[1]\n",
        "\n",
        "  elif (model_score.loc['ROC Value']['Best Score'] == 'RandomForestClassifier'):\n",
        "    best_model = fitted_models[2]\n",
        "    best_roc_value = model_score.loc['ROC Value']['RandomForestClassifier']\n",
        "    best_preds = predictions[2]\n",
        "\n",
        "  elif (model_score.loc['ROC Value']['Best Score'] == 'XGBClassifier'):\n",
        "    best_model = fitted_models[3]\n",
        "    best_roc_value = model_score.loc['ROC Value']['XGBClassifier']\n",
        "    best_preds = predictions[3]\n",
        "    \n",
        "  elif (model_score.loc['ROC Value']['Best Score'] == 'AdaBoostClassifier'):\n",
        "    best_model = fitted_models[4]\n",
        "    best_roc_value = model_score.loc['ROC Value']['AdaBoostClassifier']\n",
        "    best_preds = predictions[4]\n",
        "\n",
        "  elif (model_score.loc['ROC Value']['Best Score'] == 'LGBMClassifier'):\n",
        "    best_model = fitted_models[5]\n",
        "    best_roc_value = model_score.loc['ROC Value']['LGBMClassifier']\n",
        "    best_preds = predictions[5]  \n",
        "\n",
        "  def bestModel(params):\n",
        "    if(best_model == fitted_models[0]):\n",
        "      clf = GaussianNB(**params)\n",
        "    elif (best_model == fitted_models[1]):\n",
        "      clf = LogisticRegression(**params,verbose=0)\n",
        "    elif (best_model == fitted_models[2]):\n",
        "      clf = RandomForestClassifier(**params)\n",
        "    elif (best_model == fitted_models[3]):\n",
        "      clf = xgb.XGBClassifier(**params)\n",
        "    elif (best_model == fitted_models[4]):\n",
        "      clf = AdaBoostClassifier(**params)\n",
        "    elif (best_model == fitted_models[5]):\n",
        "      clf = lgb.LGBMClassifier(**params)\n",
        "\n",
        "    return clf\n",
        "\n",
        "  def objective (params, n_folds =10, X_train=X_train,y_train=y_train):\n",
        "    clf = Pipeline(\n",
        "      steps=[('dropcorr',DropCorrelation(0.9)),\n",
        "            ('features',SelectKBest(mutual_info_classif, k=20)),\n",
        "            ('models', bestModel(params))]\n",
        "    )\n",
        "    scores = cross_val_score(clf, preprocessor.fit_transform(X_train, y_train), y_train,cv=5,scoring='roc_auc')\n",
        "    best_scores = max(scores)\n",
        "    loss = 1-best_scores\n",
        "    return{'loss':loss, 'params':params, 'status':STATUS_OK}\n",
        "\n",
        "  if (best_model == fitted_models[0]):\n",
        "    space={\n",
        "        'var_smoothing' : hp.uniform('var_smoothing',np.logspace(0, -9, num=100))\n",
        "    }\n",
        "  elif (best_model == fitted_models[1]):\n",
        "    space = {\n",
        "          'warm_start': hp.choice('warm_start', [True, False]),\n",
        "          'fit_intercept': hp.choice('fit_intercept', [True, False]),\n",
        "          'tol': hp.uniform('tol', 0.00001, 0.0001),\n",
        "          'C': hp.uniform('C', 0.05, 3),\n",
        "          'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear','sag']),\n",
        "          'max_iter': hp.choice('max_iter', range(1000, 5000)),\n",
        "          'multi_class': 'auto',\n",
        "          'class_weight': hp.choice('class_weight',[None,'balanced'])\n",
        "          }\n",
        "  elif (best_model == fitted_models[2]):\n",
        "    space = {\n",
        "          'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
        "          'max_depth': hp.quniform('max_depth', 1, 10, 1),\n",
        "          'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
        "          'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.5),\n",
        "          'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
        "          'n_estimators' : hp.choice('n_estimators', range(10, 1000))\n",
        "      }\n",
        "  elif (best_model == fitted_models[3]):\n",
        "    space = {\n",
        "          'booster': hp.choice('booster', ['gbtree','gblinear','dart']),\n",
        "          'max_depth': hp.choice('max_depth', np.arange(1, 10, dtype=int)),\n",
        "          'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n",
        "          'gamma': hp.uniform('gamma', 0, 1),\n",
        "          'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
        "          'scale_pos_weight': hp.quniform('scale_pos_weight', 1,8,1),\n",
        "          'subsample' : hp.uniform('subsample',0.3,0.8),\n",
        "          'colsample_bytree' : hp.uniform('colsample_bytree',0.3,0.8),\n",
        "          'class_weight': hp.choice('class_weight',[None,'balanced'])\n",
        "      }\n",
        "  elif (best_model == fitted_models[4]):\n",
        "    space = {\n",
        "          'n_estimators': hp.choice('n_estimators', range(500, 1000)),\n",
        "          'learning_rate' : hp.uniform('learning_rate', 0.01, 1)\n",
        "      }\n",
        "  elif (best_model == fitted_models[5]):\n",
        "    space = {\n",
        "          'is_unbalance': hp.choice('is_unbalance', [True, False]),\n",
        "          'max_depth': hp.choice('max_depth', np.arange(3, 10, dtype=int)),\n",
        "          'boosting_type': hp.choice('boosting_type', ['gbdt','dart','goss']),\n",
        "          'num_leaves': hp.choice('num_leaves', np.arange(30, 150, dtype=int)),\n",
        "          'learning_rate': hp.uniform('learning_rate', 0.0001, 1),\n",
        "          'max_bin': hp.choice('max_bin', np.arange(50, 500, dtype=int)),\n",
        "          'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
        "          'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
        "          'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n",
        "      }\n",
        "  \n",
        "  tpe_algor = tpe.suggest\n",
        "  trials = Trials()\n",
        "  best = fmin(fn=objective, space = space,algo=tpe.suggest,max_evals=10, trials = trials)\n",
        "  trials_dict = sorted(trials.results, key = lambda x: x['loss']) \n",
        "  hyp_df = pd.DataFrame(trials_dict)\n",
        "  best_hp = hyp_df.loc[0,'params']\n",
        "  hp_model = bestModel(best_hp)\n",
        "\n",
        "  pipe2 = Pipeline(\n",
        "    steps=[('preprocessor',preprocessor),\n",
        "          ('dropcorr',DropCorrelation(0.9)),\n",
        "          ('features',SelectKBest(mutual_info_classif, k=20)),\n",
        "          ('model', hp_model)]\n",
        "  )\n",
        "\n",
        "  hyper_model = pipe2.fit(X_train, y_train)\n",
        "\n",
        "  hp_preds = hyper_model.predict(X_test)\n",
        "  hp_probs = hyper_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "  hp_roc_value = roc_auc_score(y_test, hp_probs)\n",
        "\n",
        "  if (best_roc_value > hp_roc_value):\n",
        "    fin_model = best_model\n",
        "    fin_preds = best_preds\n",
        "    fin_roc_value = best_roc_value\n",
        "  else :\n",
        "    fin_model = hyper_model\n",
        "    fin_preds = hp_preds\n",
        "    fin_roc_value = hp_roc_value\n",
        "\n",
        "  def savetotxt(filename ,model, preds):\n",
        "    with open(filename, 'w') as outfile:\n",
        "      outfile.write(\"train accuracy: {:.2f}%\\n\".format((1 - (1 - model.score(X_train, y_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))*100))\n",
        "      outfile.write(\"test accuracy: {:.2f}%\\n\".format((1 - (1 - model.score(X_test, y_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))*100))\n",
        "      outfile.write(\"roc value : {:.2f}%\\n\".format(best_roc_value*100))\n",
        "      outfile.write('\\nClassification Report\\n\\n{}\\n\\nConfusion Matrix\\n\\n{}\\n'.format(classification_report(y_test, preds),confusion_matrix(y_test, preds)))\n",
        "\n",
        "  if (file_exists(oldFile(\"finalmodel\",\"pkl\"))==True):\n",
        "      # Load Model Lama\n",
        "    old_model = joblib.load(oldFile(\"finalmodel\",\"pkl\"))\n",
        "\n",
        "    # Cek roc_value model lama\n",
        "    old_preds = old_model.predict(X_test)\n",
        "    old_probs = old_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    old_roc_value = roc_auc_score(y_test, old_probs)\n",
        "    old_train_acc = (1 - (1 - old_model.score(X_train, y_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1))) * 100\n",
        "    old_test_acc = (1 - (1 - old_model.score(X_test, y_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))) * 100\n",
        "\n",
        "    # If statement kalau roc value model lama > model baru = model lama; model baru > model lama = model baru\n",
        "    if (fin_roc_value > old_roc_value):\n",
        "      savetotxt(unique_file(\"finmetrics\",\"txt\"), fin_model,fin_preds)\n",
        "      savetotxt(unique_file(\"oldmodelmetrics\",\"txt\"),old_model, old_preds)\n",
        "      joblib.dump(fin_model, unique_file(\"finalmodel\", \"pkl\"))\n",
        "    else:\n",
        "      savetotxt(\"oldmodelmetrics.txt\",old_model, old_preds)\n",
        "      print(\"Your old model is still better than the new model...\")\n",
        "  \n",
        "  else:\n",
        "    savetotxt(unique_file(\"finmetrics\",\"txt\"), fin_model,fin_preds)\n",
        "    joblib.dump(fin_model, unique_file(\"finalmodel\", \"pkl\"))"
      ],
      "metadata": {
        "id": "nXmPef1BXNJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  fcna = FillCatsNA()\n",
        "  cd = ConvertData()\n",
        "  rvt = ReverseVarianceThreshold(threshold = 100)  \n",
        "  dc = DropCorrelation(0.9)\n",
        "  main()\n",
        "  # schedule.every(7).minutes.do(main)\n",
        "  # while True:\n",
        "  #   schedule.run_pending()\n",
        "  #   time.sleep(1)"
      ],
      "metadata": {
        "id": "my7uslQyZq6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8673cce-9881-4ec8-df2f-a7ae5419c589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DtypeWarning: Columns (42,60,65,82,83,84,85,86) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 10/10 [46:26<00:00, 278.69s/it, best loss: 0.2100546051245492]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schedule.clear()"
      ],
      "metadata": {
        "id": "pM9E6fbgnwOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}